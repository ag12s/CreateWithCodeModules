{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "CwC_Pollution_Analysis2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4v77WWzhwly"
      },
      "source": [
        "# **Pollution Analysis** (Flume Videos)\n",
        "\n",
        "This is a continuation of our analysis notebook from yesterday! Here are the differences:\n",
        "\n",
        "* We are setting up our code in such a way that we can save concentrations from multiple videos to the same DataFrame\n",
        "* We are saving our DataFrame to a CSV file for later use in another notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V6GpORvCi9k"
      },
      "source": [
        "## Step 1: Install the `pyvideoreader` package\n",
        "\n",
        "This package contains `videoreader`. From `videoreader`, we will import `VideoReader` to read in video data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDAl-AbQnmyf"
      },
      "source": [
        "#Install the required package for reading in video data\n",
        "!pip install pyvideoreader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ME8S0W9nrTA"
      },
      "source": [
        "## Step 2: Import the other packages we will use\n",
        "\n",
        "*   `numpy`\n",
        "*   `matplotlib.pyplot`\n",
        "*   `files` (for uploading files to your Colab workspace)\n",
        "*   `VideoReader` (for reading data from video frames)\n",
        "*   `plotly.express` (for creating interactive, inline figures)\n",
        "*   `cv2` (for saving frame data to image files)\n",
        "*   `pandas` (for collecting and handling our data before plotting)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RvZZtSvoDe8"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from videoreader import VideoReader\n",
        "import plotly.express as px\n",
        "import cv2\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoVDteQYpRxz"
      },
      "source": [
        "## Step 3: Download flume videos from Canvas to your computer\n",
        "\n",
        "1.   Go to this week's site on Canvas (Create with Code - Environmental Modeling)\n",
        "2.   Click on 'Modules' from the menu on the left-hand side\n",
        "3.   Scroll down to the 'Data' section\n",
        "4.   Click on the video you want to work with\n",
        "5.   Click the Download link\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ck3BR-isIJU"
      },
      "source": [
        "## Step 4: Upload the video to Colab\n",
        "\n",
        "1.   Run the cell below\n",
        "2.   Click on `'Choose Files'`\n",
        "3.   Select the video file from your computer\n",
        "4.   Wait for the video to upload to your workspace\n",
        "\n",
        "NOTE: If you close this tab, you will have to upload the video again. However, if you restart the kernel, the video will still be in your workspace. You can skip this cell once you have already uploaded the video or press `'Cancel'` when asked to upload a video after running it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8sNK3nJsk1s"
      },
      "source": [
        "# This command allows you to upload a file from your computer to the cloud where this notebook exists\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csFqJMw6tgh6"
      },
      "source": [
        "## Step 5: Read in the video file with `VideoReader` \n",
        "\n",
        "We use VideoReader to read in frames from our video. After doing so, we will print the following information:\n",
        "\n",
        "*   video file name\n",
        "*   number of frames\n",
        "*   size of the frames\n",
        "*   frame rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNWBip_NuEOC"
      },
      "source": [
        "# !!! Insert the name of the video you are uploading below BEFORE uploading the file to avoid errors. !!! \n",
        "videoName = 'NoObs_0p1mpers_short.mp4'\n",
        "video_file = VideoReader(videoName)\n",
        "\n",
        "print(video_file)  # Prints video file name, number of frames, frame size, and frame rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdmEALaqucPF"
      },
      "source": [
        "The size `(720, 1280, 3)` tells us that each frame has a height of `720` pixels and width of `1280` pixels. The `3` tells us that each image has three channels; these are the color channels! The first one is **red**, the second is **green**, and the third is **blue** (Have you heard of RGB color values?).\n",
        "\n",
        "The frame rate of `30.00 fps` tells us that thirty frames are taken every second (`fps` = frames per second)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX1-ToRmxT7A"
      },
      "source": [
        "## Step 6: Convert the video file to individual image files\n",
        "\n",
        "We will use methods from `cv2` to read a video file, then loop through each frame in the video and save it as a JPEG image file. This allows us to step through time, in a sense, and analyze the video frames independently of each other.\n",
        "\n",
        "The cell below is set up to load 10 frames (`end = 10`). To load a different number of frames, change `end =` to another number. As a reminder, you cannot load in more frames than the number that exist in the video. You can find the number of available frames by looking at the output from the last cell (our video has 54 frames in total)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsZn_Rs4xc8w"
      },
      "source": [
        "file_count = 0\n",
        "start = 1\n",
        "end = 5 # Replace this number with however many frames you want to work with\n",
        "\n",
        "frame_count = 0\n",
        "cap2 = cv2.VideoCapture(videoName)\n",
        "# While the video file is open, we iterate through the video, extract the frames, and save them to a JPEG image file \n",
        "while (cap2.isOpened()):\n",
        "    if frame_count >= start and frame_count < (end+1):\n",
        "        success,image = cap2.read()\n",
        "        if success == False:\n",
        "            break\n",
        "        cv2.imwrite('Frame_'+str(frame_count)+'.jpg', image) # save frame as JPEG file      \n",
        "        print('Read a new frame: ', success)\n",
        "    frame_count += 1\n",
        "    if frame_count == (end+1): # Close video file once frames have been retrieved\n",
        "        cap2.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ0WnkpH2ERU"
      },
      "source": [
        "The printed values `True` above tell us that each frame was read successfully."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDONDa_iA54d"
      },
      "source": [
        "## Manipulating the images\n",
        "\n",
        "Now that we have loaded in the frames we want to work with, we can start using them!\n",
        "\n",
        "To get a better idea of what we are working with, let's output the first frame of our video:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V2o0eLa2n4I"
      },
      "source": [
        "# Read the frame\n",
        "img_rgb = plt.imread('Frame_1.jpg')\n",
        "\n",
        "# Show the frame\n",
        "fig = px.imshow(img_rgb)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nrq0VmBEB8WJ"
      },
      "source": [
        "In this image, we can see the plume, a tube of dye to the right, and a piece of paper toward the top of the image. Out of these, we are really **only interested in the plume**. So, let's crop the image to just look at the plume!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwxSnT44DLDb"
      },
      "source": [
        "We don't want to include the tube of dye in our cropped image because it will skew our concentration calculations later on. Since we want to crop this out, we can choose a section of the image more toward the center.\n",
        "\n",
        "Since the image file is read in as a matrix, we can use Python's **list slicing** capabilities to crop it. We can mouse over the image displayed in the previous cell's output to pick the indices we will use to crop the image.\n",
        "\n",
        "Let's crop the vertical axis (rows) between `350` and `600`, and the horizonal axis (columns) between `300` and `1000`. The slicing syntax looks like:\n",
        "\n",
        "```\n",
        "cropped_image = image[350:600, 300:1000, :]\n",
        "```\n",
        "\n",
        "We pass `:` into the third entry because we want to keep all three color channels (red, green, blue).\n",
        "\n",
        "I want to save these values to variables so it is easier to make changes later on. Setting `row_min = 350`, `row_max = 600`, `col_min = 300`, and `col_max = 1000` gives us the following syntax:\n",
        "\n",
        "```\n",
        "cropped_image = image[row_min:row_max, col_min:col_max, :]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyjV_kUW1uxg"
      },
      "source": [
        "# Crop the image\n",
        "row_min = 350\n",
        "row_max = 600\n",
        "\n",
        "col_min = 300\n",
        "col_max = 1000\n",
        "\n",
        "img_rgb_crop = img_rgb[row_min:row_max, col_min:col_max, :]\n",
        "\n",
        "# Display the cropped image\n",
        "fig = px.imshow(img_rgb_crop)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGgVdy10Jv5k"
      },
      "source": [
        "If the cropped image above looks good to you, then we can go ahead and crop the rest of the frames we are working with! If you want to make changes, do so before running the next cell by editing the `row_min`, `row_max`, `col_min`, and `col_max` values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qeog4vYWOW6A"
      },
      "source": [
        "Each cropped image will get added to a `list` in a **for loop**. Then, the `list` will be converted to a `tuple` and passed into NumPy's `stack()` method. This method stacks the images in order; we do this so that we have one variable to work with rather than many."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcUnEJQeO9q1"
      },
      "source": [
        "# Create an empty list to store the frames in\n",
        "all_frames = []\n",
        "\n",
        "for frame_count in range(end):  # 'end' contains the number of frames we chose to read in earlier\n",
        "  all_frames.append(plt.imread('Frame_'+str(frame_count+1)+'.jpg')[row_min:row_max, col_min:col_max, :])\n",
        "\n",
        "all_frames = tuple(all_frames)\n",
        "\n",
        "img_rgb = np.stack(all_frames, axis = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ALWIu9iM2ZH"
      },
      "source": [
        "Lastly, let's **splice** our stack into three separate matrices, one for each **color channel**.\n",
        "\n",
        "Red is stored in the first channel, green is stored in the second, and blue is stored in the third. Since Python begins indexing at `0`, this means red can be found at index `0`, green can be found at index `1`, and blue can be found at index `2` of the color channels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdeRgCnxSK3x"
      },
      "source": [
        "# Save the red channel to a new variable\n",
        "Red = img_rgb[:,:,0,:]\n",
        "\n",
        "# Save the green channel to a new variable\n",
        "Green = img_rgb[:,:,1,:]\n",
        "\n",
        "# Save the blue channel to a new variable\n",
        "Blue = img_rgb[:,:,2,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VEklZ1TSkvj"
      },
      "source": [
        "With this step, we are done setting up our data and can move on to analysis!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSuCQLuQ2n4J"
      },
      "source": [
        "## Splitting the plume into regions based on concentration\n",
        "\n",
        "We are ready to **separate our images into regions** based on low, medium, and high **concentration**. We can think of these regions as light orange, medium orange, and dark orange, respectively, in the frame shown above. When the dye is **more dispersed**, it appears **lighter** and has a **lower concentration**. When the dye is **less dispersed**, it appears **darker** and has a **higher concentration**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O3WtlWoUB0n"
      },
      "source": [
        "To pick color values that match low, medium, and high concentration, run your cursor over the plotted frame above. Make note of the RGB values that appear for light, medium, and dark regions.\n",
        "\n",
        "We do not need to explicitly define medium values below -- anything that is between the light and dark color values will be placed in the 'medium' category.\n",
        "\n",
        "We define some sample values here, but feel free to change them to match what you see in the frame you are working with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsnk2yUL2n4J"
      },
      "source": [
        "# Color (RGB) values for dark concentration\n",
        "dark_red = 190 \n",
        "dark_green = 90  \n",
        "dark_blue = 40  \n",
        "\n",
        "# Color (RGB) values for light concentration\n",
        "light_red = 210\n",
        "light_green = 125\n",
        "light_blue = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1TWrCm49CBR"
      },
      "source": [
        "We want to figure out which pixels in each cropped image fall into the light, medium, or dark concentration categories. Some pixels may not fall into any of them, and that is okay, too! It just means that those pixels are not counted as part of our plume. For example, pixels showing the background or floor of the flume likely won't be counted as part of our plume (and they shouldn't be, so this is good)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAfBimcxUZHY"
      },
      "source": [
        "If the pixel in question fits into a light, medium, or dark category, we need to store this information. Let's first **preallocate** some space to do that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTA0d-IsUmAW"
      },
      "source": [
        "# Preallocating arrays that are the same size as our frames in the stack, but full of 0s\n",
        "light = np.zeros(Red.shape)\n",
        "med = np.zeros(Red.shape)\n",
        "dark = np.zeros(Red.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLuVQT_sVNC5"
      },
      "source": [
        "print(light)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK7Iv4dEYgT_"
      },
      "source": [
        "We see below that we have **nested loops** -- the outermost loop (`k`) goes through each of our frames, while the inner loops (`i` and `j`) go over the pixels in each image. `i` specifies the row number of a pixel, and `j` specifies the column number.\n",
        "\n",
        "In the cell below, we decide which cateogory each pixel falls under."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxkL5ISPjqgi"
      },
      "source": [
        "# Separate colors by dark, medium, and light color ranges for all three color channels\n",
        "\n",
        "for k in range(np.size(Red,2)): # Loop over frames (time)\n",
        "  for i in range(np.size(Red, 0)): # Loop over rows (vertical)\n",
        "    for j in range(np.size(Red, 1)): # Loop over columns (horizontal)\n",
        "\n",
        "      # Check whether pixel satisfies dark conditions\n",
        "      if (Red[i,j,k] <= dark_red) and (Green[i,j,k] <= dark_green) and (Blue[i,j,k] <= dark_blue):\n",
        "        dark[i,j,k] = 1\n",
        "      \n",
        "      # Check whether pixel satisfies medium conditions\n",
        "      elif (dark_red < Red[i,j,k] < light_red) and (dark_green < Green[i,j,k] < light_green) and (dark_blue < Blue[i,j,k] < light_blue):\n",
        "        med[i,j,k] = 1\n",
        "      \n",
        "      # Check whether pixel satisfies light conditions\n",
        "      elif (Red[i,j,k] >= light_red) and (Green[i,j,k] >= light_green) and (Blue[i,j,k] >= light_blue):\n",
        "        light[i,j,k] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWjf41aD-YtS"
      },
      "source": [
        "This leaves us with three matrices: `light`, `med`, and `dark`. These matrices have a `1` wherever a pixel fits their color/concentration requirements, and a `0` everywhere else. So, a pixel is only in the dark matrix if its red, green, *and* blue values are darker than the specified thresholds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANjV4A8RZIZ7"
      },
      "source": [
        "## It's time ... **finding the concentration!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcPbIL4xnNet"
      },
      "source": [
        "We just made some big changes to our data! To make sure we all understand the variables we are now working with, let's print out the dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvTfrmNjnC76"
      },
      "source": [
        "print('img_rgb:')\n",
        "print('Number of rows per frame:', np.size(img_rgb,0))\n",
        "print('Number of columns per frame:', np.size(img_rgb,1))\n",
        "print('Number of channels per frame:', np.size(img_rgb,2))\n",
        "print('Number of frames:', np.size(img_rgb,3))\n",
        "\n",
        "print('\\nlight:')\n",
        "print('Number of rows per frame:', np.size(light,0))\n",
        "print('Number of columns per frame:', np.size(light,1))\n",
        "print('Number of frames:', np.size(light,2))\n",
        "\n",
        "print('\\nmed:')\n",
        "print('Number of rows per frame:', np.size(med,0))\n",
        "print('Number of columns per frame:', np.size(med,1))\n",
        "print('Number of frames:', np.size(med,2))\n",
        "\n",
        "print('\\ndark:')\n",
        "print('Number of rows per frame:', np.size(dark,0))\n",
        "print('Number of columns per frame:', np.size(dark,1))\n",
        "print('Number of frames:', np.size(dark,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPWrqyU1oTg-"
      },
      "source": [
        "Notice how the `light`, `med`, and `dark` matrices **no longer have three color channels**. This is because we do not need to store the color of a pixel -- we just need to know whether it fits in that category (`1`) or not (`0`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8egM-UcQ-zI"
      },
      "source": [
        "Now, to find the concentration, we want to store the **number of light/medium/dark points in each frame**.\n",
        "\n",
        "Just like how we preallocated storage space for `light`, `med`, and `dark`, we want to preallocated space for these numbers. Let's do that first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QcqbMwftXMG"
      },
      "source": [
        "# Save the column and frame dimensions to variables so we don't have to recalculate several times\n",
        "num_cols = np.size(img_rgb,1)\n",
        "num_frames = np.size(img_rgb,3)\n",
        "\n",
        "# Initialize arrays for storage\n",
        "num_dark = np.zeros((num_cols, num_frames))\n",
        "num_med = np.zeros((num_cols, num_frames))\n",
        "num_light = np.zeros((num_cols, num_frames))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNcdAuCpvCyb"
      },
      "source": [
        "Now, we can find values to store in these preallocated spaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPX5FNOTgdwy"
      },
      "source": [
        "# Loop over the columns\n",
        "for i in range(num_cols): \n",
        "  # Loop over the frames \n",
        "  for j in range(num_frames):\n",
        "    \n",
        "    # Find the indices of nonzero pixels in the light, med, and dark matrices (places where pixels satisfy their conditions)\n",
        "    # Previously, we saved pixels that satisfy their conditions as 1s, and everything else is 0\n",
        "    # Becaues of this, we can look for all pixel with a value greater than 0 using NumPy's where() method\n",
        "    dark_nonzero_idx = list(zip(*np.where(dark[:,:,j] > 0)))\n",
        "    med_nonzero_idx = list(zip(*np.where(med[:,:,j] > 0)))\n",
        "    light_nonzero_idx = list(zip(*np.where(light[:,:,j] > 0)))\n",
        "\n",
        "    # To get the number of pixels with each concentration, we count how many indices were found with the len() method\n",
        "    num_dark[i,j] = len(dark_nonzero_idx)\n",
        "    num_med[i,j] = len(med_nonzero_idx)\n",
        "    num_light[i,j] = len(light_nonzero_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FuBWE5G4LB1"
      },
      "source": [
        "What we have now are three matrices that store the number of low/medium/high concentration pixels per column for all of our frames:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47wsd6qF4HC9"
      },
      "source": [
        "print('\\nnum_light:')\n",
        "print('Number of entries per frame:', np.size(num_light,0))\n",
        "print('Number of frames:', np.size(num_light,1))\n",
        "\n",
        "print('\\nnum_med:')\n",
        "print('Number of entries per frame:', np.size(num_med,0))\n",
        "print('Number of frames:', np.size(num_med,1))\n",
        "\n",
        "print('\\nnum_dark:')\n",
        "print('Number of entries per frame:', np.size(num_dark,0))\n",
        "print('Number of frames:', np.size(num_dark,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20mqnm7U-GqF"
      },
      "source": [
        "Lastly, let's calculate the **average** number of light/medium/dark pixels per frame. This gives us the low/medium/high concentration in every frame! To find the average, we use NumPy's `mean()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_gwp4vwiEZY"
      },
      "source": [
        "# Take the average of each concentration\n",
        "light_mean = np.mean(num_light, axis = 0)\n",
        "med_mean = np.mean(num_med, axis = 0)\n",
        "dark_mean = np.mean(num_dark, axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlxOkY_g7XbT"
      },
      "source": [
        "After calculating the averages, we divide each one by the maximum value (number of points) from that category's frames. To find the maximum, we use NumPy's `max()` method.\n",
        "\n",
        "Doing this means we are **normalizing** the values; essentially, we are putting all three categories on the same scale so that we can make comparisons between them. After normalizing, all values should be between 0 and 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koJhzQG07aGJ"
      },
      "source": [
        "# Normalize each mean\n",
        "light_mean_norm = light_mean/np.max(num_light)\n",
        "med_mean_norm = med_mean/np.max(num_med)\n",
        "dark_mean_norm = dark_mean/np.max(num_dark)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0SU-xpYQSIg"
      },
      "source": [
        "## Visualizing Results\n",
        "\n",
        "What we have now are three variables, `light_mean_norm`, `med_mean_norm`, and `dark_mean_norm`, that hold the **low, medium, and high concentrations for each frame**. For example, if we used 10 frames, then each of these variables is a NumPy array with 10 entries. The first entry has the concentration in the first frame, and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3whfjjtQ8fzQ"
      },
      "source": [
        "print('\\nlight_mean_norm:')\n",
        "print('Type:', type(light_mean_norm))\n",
        "print('Number of entries:', np.size(light_mean_norm,0))\n",
        "\n",
        "print('\\nmed_mean_norm:')\n",
        "print('Type:', type(med_mean_norm))\n",
        "print('Number of entries:', np.size(med_mean_norm,0))\n",
        "\n",
        "print('\\ndark_mean_norm:')\n",
        "print('Type:', type(dark_mean_norm))\n",
        "print('Number of entries:', np.size(dark_mean_norm,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2U_nlks8v9t"
      },
      "source": [
        "Let's **visualize** our results! To make plotting and referencing the data simpler, we can load our arrays into a **Pandas DataFrame** first. Recall that we first store the arrays in a dictionary, then pass the dictionary to the DataFrame constructor:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF-VU6tuY5Yu"
      },
      "source": [
        "**The first time you make a DataFrame, run this cell:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO1Bcipd9ina"
      },
      "source": [
        "# Creating an array with frame numbers\n",
        "frames = np.linspace(1, end, num = len(light_mean_norm), axis = 0)\n",
        "\n",
        "# Creating a dictionary with keys 'Low', 'Medium', and 'High' (referring to concentration levels), and 'Frame' (frame numbers)\n",
        "# !!! Make sure you use correct key names !!!\n",
        "data_to_plot = {'Frame': frames, 'Low_noObs_0p1': light_mean_norm, 'Medium_noObs_0p1': med_mean_norm, 'High_noObs_0p1': dark_mean_norm}\n",
        "\n",
        "# Passing our dictionary to the DataFrame constructor\n",
        "df = pd.DataFrame(data=data_to_plot)\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDZ9YBFJY92W"
      },
      "source": [
        "**After the DataFrame has already been created, run this cell instead:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y9mBr0d-e_t"
      },
      "source": [
        "# Creating a dictionary with keys 'Low', 'Medium', and 'High' (referring to concentration levels), and 'Frame' (frame numbers)\n",
        "# !!! Make sure you use correct key names !!!\n",
        "data_to_plot = {'Frame': frames, 'Low_noObs_0p4': light_mean_norm, 'Medium_noObs_0p4': med_mean_norm, 'High_noObs_0p4': dark_mean_norm}\n",
        "\n",
        "# Passing our dictionary to the DataFrame constructor\n",
        "df2 = pd.DataFrame(data=data_to_plot)\n",
        "\n",
        "df = df.merge(df2, how = 'inner', on = 'Frame')\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_e5PHzF-y6M"
      },
      "source": [
        "Last step -- let's plot each of these!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGt6codYl2nA"
      },
      "source": [
        "# Visualize the high concentration data\n",
        "df.plot(kind = 'scatter', x = 'Frame', y = 'High', title = 'High concentrations over time', color = 'k')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsbd0yu6PGZ-"
      },
      "source": [
        "# Visualize the medium concentration data\n",
        "df.plot(kind = 'scatter', x = 'Frame', y = 'Medium', title = 'Medium concentrations over time', color = 'k')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtlnhfFVPVn1"
      },
      "source": [
        "# Visualize the low concentration data\n",
        "df.plot(kind = 'scatter', x = 'Frame', y = 'Low', title = 'Low concentrations over time', color = 'k')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9Gw1MFia5pi"
      },
      "source": [
        "## Exporting your data to a CSV file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfPo5qxAa7f-"
      },
      "source": [
        "df.to_csv('flume_concentrations.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzTNuqjabEMC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}